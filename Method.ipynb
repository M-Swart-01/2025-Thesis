{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37899b8",
   "metadata": {},
   "source": [
    "# Problem Setup and Configuration\n",
    "We consider the problem of simple harmonic motion with the governing equation for displacement:\n",
    "\n",
    "$$x = A \\cos(\\omega t + \\phi)$$\n",
    "$$ \\omega = sqrt(k/m)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d628e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math, torch\n",
    "import data\n",
    "import model\n",
    "import train\n",
    "import evaluation\n",
    "from config import Config\n",
    "\n",
    "def to_dtype(s): return {\"float32\": torch.float32, \"float64\": torch.float64}[s]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config()                               # 1) pick defaults\n",
    "    device = cfg.device\n",
    "    dtype  = to_dtype(cfg.dtype_str)\n",
    "\n",
    "    # 2) make dataset\n",
    "    t_data, x_data, _ = data.data_generation(cfg, device, dtype)\n",
    "\n",
    "    # 3) compute initial conditions from (A, phi) -> (x0, v0)\n",
    "    w = math.sqrt(cfg.k/cfg.m)\n",
    "    x0 = cfg.A * math.cos(cfg.phi)\n",
    "    v0 = -cfg.A * w * math.sin(cfg.phi)\n",
    "\n",
    "    # 4) build model\n",
    "    net = model.build_model(cfg, device, dtype, x0, v0)\n",
    "\n",
    "    # 5) train\n",
    "    net = train.train(net, cfg, t_data, x_data, device, dtype)\n",
    "\n",
    "    # 6) evaluate / plot\n",
    "    evaluation.evaluate_and_plot(net, cfg, t_data, x_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460fc01",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "- A time vector is created between t_start and t_end, with N evenly spaced points.\n",
    "- An exact solution is computed from the equation above.\n",
    "- Gaussian noise with standard deviation noise_std is added to simulate sensor noise.\n",
    "\n",
    "## Notes:\n",
    "- omega is calculated here\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd93c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, numpy as np, torch\n",
    "\n",
    "def omega(cfg): \n",
    "    # Calculate angular frequency for use in exact solution\n",
    "    return math.sqrt(cfg.k / cfg.m)\n",
    "\n",
    "def true_x(cfg, t_np: np.ndarray) -> np.ndarray:\n",
    "    # Exact solution for simple harmonic motion\n",
    "    return cfg.A * np.cos(omega(cfg) * t_np + cfg.phi)\n",
    "\n",
    "def data_generation(cfg, device, dtype):\n",
    "    N = getattr(cfg, \"N_data\", 200)\n",
    "    seed = getattr(cfg, \"seed\", None)\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)     # !!!!!!!!!!! controls NumPy's random numbers (noise)\n",
    "        torch.manual_seed(seed)  # !!!!!!!!!!! controls PyTorch RNG (not strictly needed here, but good practice)\n",
    "    t = np.linspace(cfg.t_start, cfg.t_end, N)  # NumPy time vector\n",
    "    noise = cfg.noise_std * np.random.randn(len(t)) # ... if cfg.use_data_loss else 0.0\n",
    "    x_clean = true_x(cfg, t)    # analytical solution\n",
    "    x_noisy = x_clean + noise   # noisy measurements\n",
    "    t_data = torch.tensor(t, device=device, dtype=dtype).view(-1,1)     # time vector in tensor form\n",
    "    x_data = torch.tensor(x_noisy, device=device, dtype=dtype).view(-1,1)   # noisy measurements in tensor form\n",
    "    return t_data, x_data, x_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0523858",
   "metadata": {},
   "source": [
    "# PINN Model\n",
    "Next, a standard multilayer perceptron (MLP) is constructed based on previously defined hyperparameters.\n",
    "- fully connected neural net (MLP)\n",
    "- input = time; output = displacement prediction x(t)\n",
    "- this is where the hyperparameters come into play (activation functions, network depth and width, init (xavier))\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "def get_activation(name):                                                       # get activation function by name from config\n",
    "    return {\"tanh\": nn.Tanh(), \"relu\": nn.ReLU(), \"silu\": nn.SiLU()}[name]      # table/dictionary lookup\n",
    "\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)                                       # Xavier initialization to weights ********* add he\n",
    "        nn.init.zeros_(m.bias)                                                  # set biases to zero\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg, x0, v0, device, dtype):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hard_ics = cfg.hard_ics                                        # boolean, enforching hard ics or not\n",
    "        self.x0 = torch.tensor([[x0]], device=device, dtype=dtype)\n",
    "        self.v0 = torch.tensor([[v0]], device=device, dtype=dtype)          # !!!!!!!!!!! initial position and velocity as tensors?\n",
    "\n",
    "        act = get_activation(cfg.activation)\n",
    "\n",
    "        layers = [nn.Linear(1, cfg.hidden_width), act]                      # input layer \n",
    "        for _ in range(cfg.hidden_layers-1):                                # hidden layers\n",
    "            layers += [nn.Linear(cfg.hidden_width, cfg.hidden_width), act]\n",
    "        layers += [nn.Linear(cfg.hidden_width, 1)]                          # output layer\n",
    "        self.net = nn.Sequential(*layers)         \n",
    "        if cfg.init == \"xavier\": self.net.apply(xavier_init)                # apply to submodules\n",
    "\n",
    "    def forward(self, t):\n",
    "        raw = self.net(t)                                                       \n",
    "        return self.x0 + self.v0*t + (t**2)*raw if self.hard_ics else raw   # enforce hard ics if specified     \n",
    "\n",
    "def build_model(cfg, device, dtype, x0, v0):                                     \n",
    "    return MLP(cfg, x0, v0, device, dtype).to(device).to(dtype)             # redundant to .to(dtype) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d625e",
   "metadata": {},
   "source": [
    "# Training\n",
    "Physics constraints are integrated into the training function by incorporating a loss function for initial conditions and governing equations alongside the standard data loss.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "def ddt(y, t):      # first derivative of *******y wrt t, returns gradient, creates graph for higher derivatives\n",
    "    return torch.autograd.grad(y, t, torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "    # return torch.autograd.grad(\n",
    "    #     outputs=y, inputs=t,\n",
    "    #     grad_outputs=torch.ones_like(y),\n",
    "    #     create_graph=True\n",
    "    # )[0]\n",
    "\n",
    "def d2dt2(y, t):    # second derivative of y wrt t (repeat ddt())\n",
    "    return ddt(ddt(y, t), t)\n",
    "\n",
    "_MSE = nn.MSELoss()\n",
    "\n",
    "# !!!!!! collocation sampling, check with original\n",
    "def sample_times(cfg, device, dtype):\n",
    "    t = torch.rand(cfg.n_phys_per_step,1, device=device, dtype=dtype)*(cfg.t_end-cfg.t_start)+cfg.t_start\n",
    "    t.requires_grad_(True)\n",
    "    return t\n",
    "\n",
    "# ---- loss functions ----\n",
    "def physics_loss(model, cfg, t_phys):\n",
    "    x = model(t_phys)\n",
    "    x_tt = d2dt2(x, t_phys)\n",
    "    return _MSE(x_tt + (cfg.k/cfg.m)*x, torch.zeros_like(x))\n",
    "\n",
    "def ic_loss(model, cfg, device, dtype):\n",
    "    if getattr(model, \"hard_ics\", False):\n",
    "        return torch.tensor(0.0, device=device, dtype=dtype)\n",
    "    t0 = torch.zeros(1,1, device=device, dtype=dtype).requires_grad_(True)\n",
    "    x0 = model.x0; v0 = model.v0\n",
    "    x_pred = model(t0); v_pred = ddt(x_pred, t0)\n",
    "    return _MSE(x_pred, x0) + _MSE(v_pred, v0)\n",
    "\n",
    "def data_loss(model, t_data, x_data):\n",
    "    return _MSE(model(t_data), x_data)\n",
    "\n",
    "# ---- training loop ----\n",
    "def train(model, cfg, t_data, x_data, device, dtype):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr)                 # !!!! Optimised - make configurable?\n",
    "    \n",
    "    curve_total, curve_phys, curve_ic, curve_data = [], [], [], []\n",
    "    t0 = time.time()  # start timer to log train_time_min\n",
    "\n",
    "    for step in range(1, cfg.steps+1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        t_phys = sample_times(cfg, device, dtype)\n",
    "        l_phys = physics_loss(model, cfg, t_phys)\n",
    "        l_ic   = ic_loss(model, cfg, device, dtype)\n",
    "        l_data = data_loss(model, t_data, x_data) if cfg.use_data_loss else torch.tensor(0.0, device=device, dtype=dtype)\n",
    "        loss = cfg.w_phys*l_phys + cfg.w_ic*l_ic + cfg.w_data*l_data # total loss\n",
    "\n",
    "        loss.backward(); opt.step() # backprop using previously defined optimiser\n",
    "        \n",
    "        # logging\n",
    "        curve_total.append(loss.item())\n",
    "        curve_phys.append(l_phys.item())\n",
    "        curve_ic.append(l_ic.item())\n",
    "        curve_data.append(l_data.item())\n",
    "\n",
    "\n",
    "        if step % cfg.print_every == 0:\n",
    "            print(f\"[{step:4d}] total={loss.item():.3e} phys={l_phys.item():.3e} ic={l_ic.item():.3e} data={l_data.item():.3e}\")\n",
    "        # End stats\n",
    "        train_minutes = (time.time() - t0) / 60.0\n",
    "        stats = {\n",
    "            \"final_total\": curve_total[-1],\n",
    "            \"final_phys\":  curve_phys[-1],\n",
    "            \"final_ic\":    curve_ic[-1],\n",
    "            \"final_data\":  curve_data[-1],\n",
    "            \"curve_total\": curve_total,\n",
    "            \"curve_phys\":  curve_phys,\n",
    "            \"curve_ic\":    curve_ic,\n",
    "            \"curve_data\":  curve_data,\n",
    "            \"train_time_min\": train_minutes,\n",
    "        }\n",
    "    return model, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ec048",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9760bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from data import true_x\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_plot(model, cfg, t_data, x_data):\n",
    "    t_eval = torch.linspace(cfg.t_start, cfg.t_end, 801, device=cfg.device, dtype={\"float32\":torch.float32,\"float64\":torch.float64}[cfg.dtype_str]).unsqueeze(1)\n",
    "    x_pred = model(t_eval).cpu().numpy().squeeze()\n",
    "    x_true = true_x(cfg, t_eval.cpu().numpy().squeeze())\n",
    "    l2 = float(np.sqrt(np.mean((x_pred - x_true)**2)))\n",
    "    print(f\"[Eval] L2 error = {l2:.3e}\")\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(); plt.scatter(t_data.cpu(), x_data.cpu(), s=10, c='tab:red', label='data')\n",
    "        plt.plot(t_eval.cpu(), torch.tensor(x_true), 'k--', label='exact')\n",
    "        plt.plot(t_eval.cpu(), torch.tensor(x_pred), label='PINN')\n",
    "        plt.legend(); plt.tight_layout(); plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return l2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
